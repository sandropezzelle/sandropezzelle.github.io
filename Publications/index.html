
<!DOCTYPE html>
<html>
        <head>
                <title>Sandro Pezzelle</title>
                <!-- link to main stylesheet -->
                <link rel="stylesheet" type="text/css" href="/main.css">
        </head>
        <body>
                <nav>
                <ul>

                        <li><a href="/">Home</a></li>
                        <li><a href="/CV/">CV</a></li>
                        <li><a href="/Publications/">Publications</a></li>
                        <li><a href="/Presentations/">Presentations</a></li>
                        <!--
				<li><a href="/Other/">Other Projects</a></li>
				-->

                </ul>
                </nav>
                <div class="container">
                <div class="blurb">
                        <h2>Publications</h2>

<!--
                     <h2>Preprints</h2>
-->
                        <ul>

			<li>Bavaresco, A., de Heer Kloots, M., <b>Pezzelle, S.</b>, Fernández, R. <i>preprint</i>. Modelling Multimodal Integration in Human Concept Processing with Vision-and-Language Models <a href="https://arxiv.org/abs/2407.17914" target="_blank">[preprint]</a></li> <br>

                        <li>Bavaresco, A., Bernardi, R., Bertolazzi, L., Elliott, D., Fernández, R., Gatt, A., Ghaleb, E., Giulianelli, M., Hanna, M., Koller, A., Martins, A., Mondorf, P., Neplenbroek, V., <b>Pezzelle, S.</b>, Plank, B., Schlangen, D., Suglia, A., Surikuchi, A., Takmaz, E., Testoni, A. <i>(preprint)</i>. LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks <a href="https://arxiv.org/abs/2406.18403" target="_blank">[preprint]</a></li> <br>

			<li>Bai, Y. and <b>Pezzelle, S.</b> <i>(preprint)</i> If I understand the context, I will act accordingly: Combining Complementary Information with Generative Visual Language Models <a href="" target="_blank">[preprint]</a></li> <br>

                        <li>Cinà, G., Fernandez-Llaneza, D., Deponte, L., Mishra, N., Röber, T. E., <b>Pezzelle, S.</b>, Calixto, I., Goedhart, R. and Birbil, Ş. İ. <i>(preprint)</i>. Fixing confirmation bias in feature attribution methods via semantic match. <a href="https://arxiv.org/pdf/2307.00897" target="_blank">[preprint]</a></li><br>

		<li>Mehrparvar, B. and <b>Pezzelle, S.</b> <i>(preprint)</i> Detecting and Translating Language Ambiguity with Multilingual LLMs <a href="" target="_blank">[preprint]</a></li> <br>

<!--
                        <h2>2024</h2>
                        <ul>
-->

                        <li>[41] Surikuchi, A., Fernández, R., and <b>Pezzelle, S.</b> (2024). Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition. To appear in <b>Findings of EMNLP 2024</b> <a href="https://arxiv.org/abs/2407.04559" target="_blank">[preprint]</a></li>
			<br>

                        <li>[40] Hanna, M., <b>Pezzelle, S.</b>, Belinkov, Y. (2024). Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms. To appear in <b>CoLM 2024</b> <a href="https://arxiv.org/pdf/2403.17806.pdf" target="_blank">[preprint]</a></li>
			<br>

                        <li>[39] Testoni, A., Sprott, J., <b>Pezzelle, S.</b> (2024). Naming, Describing, and Quantifying Visual Objects in Humans and LLMs. <b>ACL 2024</b> <a href="https://arxiv.org/abs/2403.06935" target="_blank">[preprint]</a></li>
			<br>

                        <li>[38] Wildenburg, F., Hanna, M., <b>Pezzelle, S.</b> (2024). Do Pre-Trained Language Models Detect and Understand Semantic Underspecification? <i>Ask the DUST!</i> <b>Findings of ACL 2024</b> <a href="https://arxiv.org/abs/2402.12486" target="_blank">[preprint]</a></li>
                        <br>

		        <li>[37] Takmaz, E., <b>Pezzelle, S.</b>, and Fernández, R. (2024). Describing Images <i>Fast and Slow</i>: Quantifying and Predicting the Variation in Human Signals during Visuo-Linguistic Processes. <b>EACL 2024</b>. <a href="https://aclanthology.org/2024.eacl-long.126/" target="_blank">[paper]</a> <a href="https://aclanthology.org/2024.eacl-long.126.bib" target="_blank">[bib]</a> [code]</li>

<br>

<!--
			</ul>
			<h2>2023</h2>
			<ul>
-->


			<li>[36] Hanna, M., Belinkov, Y., and <b>Pezzelle, S.</b> (2023). When Language Models Fall in Love: Animacy Processing in Transformer Language Models. <b>EMNLP 2023</b>. <a href="https://aclanthology.org/2023.emnlp-main.744.pdf" target="_blank">[paper]</a> <a href="https://arxiv.org/abs/2310.15004" target="_blank">[preprint]</a> <a href="https://aclanthology.org/2023.emnlp-main.744.bib" target="https://aclanthology.org/2023.emnlp-main.744.bib">[bib]</a> <a href="https://github.com/hannamw/lms-in-love" target="_blank">[code&data]</a></li><br>

                        <li>[35] Chen, X., Fernández, R., and <b>Pezzelle, S.</b> (2023). The BLA Benchmark: Investigating Basic Language Abilities of Pre-Trained Multimodal Models. <b>EMNLP 2023</b>. <a href="https://aclanthology.org/2023.emnlp-main.356/" target="_blank">[paper]</a> <a href="https://aclanthology.org/2023.emnlp-main.356.bib" target="_blank">[bib]</a> <a href="https://arxiv.org/abs/2310.15061" target="_blank">[preprint]</a> <a href="https://github.com/shin-ee-chen/BLA" target="_blank">[code&data]</a></li><br>
			
			<li>[34] Surikuchi, A., <b>Pezzelle, S.</b>, and Fernández, R. (2023). GROOViST: A Metric for Grounding Objects in Visual Storytelling. <b>EMNLP 2023</b>. <a href="https://aclanthology.org/2023.emnlp-main.202/" target="_blank">[paper]</a> <a href="https://arxiv.org/abs/2310.17770" target="_blank">[preprint]</a> <a href="https://aclanthology.org/2023.emnlp-main.202.bib" target="_blank">[bib]</a> <a href="https://github.com/akskuchi/groovist" target="_blank">[code]</a></li><br>

			<li>[33] <b>Pezzelle, S.</b> (2023). Dealing with Semantic Underspecification in Multimodal NLP. <b>ACL 2023</b>. <a href="https://aclanthology.org/2023.acl-long.675/" target="_blank">[paper]</a> <a href="https://arxiv.org/pdf/2306.05240.pdf" target="_blank">[preprint]</a> <a href="https://aclanthology.org/2023.acl-long.675.bib" target="_blank">[bib]</a> <a href="https://github.com/sandropezzelle/sunglass" target="_blank">[code]</a></li><br>

			<li>[32] Takmaz, E., Brandizzi, N., Giulianelli, M., <b>Pezzelle, S.</b> and Fernández, R. (2023)</i>. Speaking the Language of Your Listener: Audience-Aware Adaptation via Plug-and-Play Theory of Mind. <b>Findings of ACL 2023</b>. <a href="https://aclanthology.org/2023.findings-acl.258/" target="_blank">[paper]</a> <a href="https://arxiv.org/pdf/2305.19933.pdf" target="_blank">[preprint]</a> <a href="https://aclanthology.org/2023.findings-acl.258.bib" target="_blank">[bib]</a> <a href="https://github.com/nicofirst1/speaker-adaptation" target="_blank">[code]</a></li><br>

			<li>[31] <b>Pezzelle, S.</b> and Fernández, R. (2023). Semantic adaptation to the interpretation of gradable adjectives via active linguistic interaction. <b>Cognitive Science</b>. <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.13248" target="_blank">[paper]</a> <a href="bib/cogsci2023.bib" target="_blank">[bib]</a> <a href="https://surfdrive.surf.nl/files/index.php/s/IUi4X4QGkUiqm9Z" target="_blank">[code]</a></li>
<br>

			<li>[30] Buijtelaar, L. and <b>Pezzelle, S.</b> (2023). A Psycholinguistic Analysis of BERT's Representations of Compounds. <b>EACL 2023</b>. <a href="https://arxiv.org/abs/2302.07232" target="_blank">[preprint]</a> <a href="https://aclanthology.org/2023.eacl-main.163.pdf" target="_blank">[paper]</a> <a href="https://aclanthology.org/2023.eacl-main.163/#" target="_blank">[bib]</a> <a href="https://github.com/lars927/compounds-analysis-bert" target="_blank">[code]</a></li>
<br>

<!--
			</ul>
			<h2>2022</h2>
			<ul>
-->

			<li>[29] Jansen, L., Laichter, S. L., Sinclair, A. , van der Goot, M., Fernández, R., <b>Pezzelle, S.</b> (2022). Controllable Text Generation for All Ages: Evaluating a Plug-and-Play Approach to Age-Adapted Dialogue. <b>GEM 2022</b> at EMNLP 2022. <a href="https://dmg-illc.github.io/ai-age-adaptation/" target="_blank">[website]</a> <a href="https://pure.uva.nl/ws/files/98482540/19_Paper.pdf" target="_blank">[paper]</a> <a href="https://aclanthology.org/2022.gem-1.14.bib" target="_blank">[bib]</a> <a href="https://github.com/lennertjansen/pplm-age-adapt-dialogue" target="_blank">[code]</a></li>
<br>

			<li>[28] Takmaz, E., <b>Pezzelle, S.</b>, Fernández, R. (2022). Less Descriptive yet Discriminative: Quantifying the Properties of Multimodal Referring Utterances via CLIP. <b>CMCL 2022</b> at ACL 2022. <a href="https://aclanthology.org/2022.cmcl-1.4/" target="_blank">[paper]</a> <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:3MB4lY9QQGUJ:scholar.google.com/&output=citation&scisdr=CgUQEzbgEL7PnZrVVFY:AAGBfm0AAAAAYw3TTFaTQ11iyEvLeHZEC9PqLXSdj7ut&scisig=AAGBfm0AAAAAYw3TTAOjr6ACsgm5Ncm2HzL-IZyiS5Tt&scisf=4&ct=citation&cd=-1&hl=it" target="_blank">[bib]</a> <a href="https://github.com/ecekt/clip-desc-disc" target="_blank">[code]</a></li>
<br>
<!--
			</ul>
			<h2>2021</h2>
			<ul>
-->


			<li>[27] <b>Pezzelle, S.</b>, Takmaz, E., Fernández, R. (2021). Word Representation Learning in Multimodal Pre-Trained Transformers: An Intrinsic Evaluation. <b>TACL</b>. <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00443/108935/Word-Representation-Learning-in-Multimodal-Pre" target="_blank">[paper]</a> <a href="bib/tacl2021.bib" target="_blank">[bib]</a> <a href="https://github.com/sandropezzelle/multimodal-evaluation" target="_blank">[github]</a></li>

<br>


			<li>[26] Jansen, L., Sinclair, A., Van der Goot, M., Fernández, R., <b>Pezzelle, S.</b> (2021). Detecting Age-Related Linguistic Patterns in Dialogue: Toward Adaptive Conversational Systems. <b>CLIC-it</b>. <a href="https://dmg-illc.github.io/ai-age-adaptation/" target="_blank">[website]</a> <a href="http://ceur-ws.org/Vol-3033/paper47.pdf" target="_blank">[paper]</a> <a href="bib/clicit2021.bib" target="_blank">[bib]</a> <a href="https://github.com/lennertjansen/detecting-age-in-dialogue" target="_blank">[github]</a></li>

<br>


			<li>[25] Van der Goot, M., Georgiou, M., Dolinšek, Š., Jansen, L., Sinclair, A., Fernández, R., <b>Pezzelle, S.</b> (2021). Exploring the potential of adapting conversational systems to different age groups: A pilot study. <b>CONVERSATIONS</b>. <a href="https://conversations2021.files.wordpress.com/2021/11/conversations_2021_positionpaper_20_vandergoot-1.pdf" target="_blank">[paper]</a> [bib]</li>

<br>


			<li>[24] Parfenova, I., Elliott, D., Fernández, R., <b>Pezzelle, S.</b> (2021). Probing Cross-Modal Representations in Multi-Step Relational Reasoning. <b>RepL4NLP 2021</b> at ACL 2021. <a href="https://aclanthology.org/2021.repl4nlp-1.16/" target="_blank">[paper]</a> <a href="https://aclanthology.org/2021.repl4nlp-1.16.bib" target="_blank">[bib]</a> <a href="https://github.com/jig-san/multi-step-size-reasoning" target="_blank">[github]</a></li>

<br>


			<li>[23] Bernardi, R., <b>Pezzelle, S.</b> (2021). Linguistic issues behind visual question answering. <b>Language and Linguistic Compass</b>. <a href="https://onlinelibrary.wiley.com/doi/10.1111/lnc3.12417" target="_blank">[paper]</a> <a href="bib/compass.bib" target="_blank">[bib]</a></li>

<br>

			<li>[22] Jolly, S., <b>Pezzelle, S.</b>, Nabi, M. (2021). EaSe: A Diagnostic Tool for VQA based on Answer Diversity. <b>NAACL-HLT 2021</b>. <a href="https://www.aclweb.org/anthology/2021.naacl-main.192.pdf" target="_blank">[paper]</a> <a href="https://aclanthology.org/2021.naacl-main.192.bib" target="_blank">[bib]</a> <a href="https://github.com/shailzajolly/EaSe" target="_blank">[github]</a></li>
<br>

<!--
			</ul>
			<h2>2020</h2>
			<ul>
-->
			<li>[21] Gualdoni, E., Bernardi, R., Fernández, R., <b>Pezzelle, S.</b> (2020). Grounded and Ungrounded Referring Expressions in Human Dialogues: Language Mirrors Different Grounding Conditions. <b>CLiC-it 2020</b>. <a href="http://ceur-ws.org/Vol-2769/paper_38.pdf" target="_blank">[paper]</a> [bib]</li>

<br>

			<li>[20] <b>Pezzelle, S.</b>, Greco, C., Gandolfi, G., Gualdoni, E., Bernardi, R. (2020). <i>Be Different to Be Better!</i> A Benchmark to Leverage the Complementarity of Language and Vision. <b>Findings of EMNLP 2020</b>. <a href="https://sites.google.com/view/bd2bb/home" target="_blank">[website]</a> <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.248.pdf" target="_blank">[paper]</a> <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.248.bib" target="_blank">[bib]</a> <a href="https://github.com/sandropezzelle/bd2bb" target="_blank">[github]</a> <a href="Pezzelle_blackboxNLP_poster.pdf">[poster]</a></li>

<br>

			<li>[19] Takmaz, E., <b>Pezzelle, S.</b>, Beinborn, L., Fernández, R. (2020). Generating Image Descriptions via Sequential Cross-Modal Alignment Guided by Human Gaze. <b>EMNLP 2020</b>. <a href="https://www.aclweb.org/anthology/2020.emnlp-main.377.pdf" target="_blank">[paper]</a> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.377.bib" target="_blank">[bib]</a> <a href="https://github.com/dmg-illc/didec-seq-gen" target="_blank">[github]</a></li>

<br>

			<li>[18] Takmaz, E., Giulianelli, M., <b>Pezzelle, S.</b>, Sinclair, A., Fernández, R. (2020). <i>Refer, Reuse, Reduce:</i> Generating Subsequent References in Visual and Conversational Contexts. <b>EMNLP 2020</b>. <a href="https://dmg-photobook.github.io/" target="_blank">[website]</a> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.353.pdf" target="_blank">[paper]</a> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.353.bib" target="_blank">[bib]</a> <a href="https://github.com/dmg-photobook/ref-gen-photobook" target="_blank">[github]</a></li>

<br>

			<li>[17] <b>Pezzelle, S.</b>, Marelli, M. (2020). Do Semantic Features Capture a Syntactic Classification of Compounds? Insights from Compositional Distributional Semantics. <b>PMWE</b>.<a href="https://langsci-press.org/catalog/view/239/1890/1764-1" target="_blank">[paper]</a> <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:_x0xPmVMqngJ:scholar.google.com/&output=citation&scisdr=CgUQ_Yf-EMjvvmGR8hE:AAGBfm0AAAAAX7OU6hEgYYGG8e9dpWyW3j6TBSBCAMQo&scisig=AAGBfm0AAAAAX7OU6se_xqipj8QHQJwIe839MoviIu6p&scisf=4&ct=citation&cd=-1&hl=it&scfhb=1" target="_blank">[bib]</a></li>

<br>

<!--
			</ul>
			<h2>2019</h2>
			<ul>
-->

                        <li>[16] <b>Pezzelle, S.</b>, Fernández, R. (2019). <i>Big</i> Generalizations with <i>Small</i> Data: Exploring the Role of Training Samples in Learning Adjectives of Size. <b>LANTERN 2019</b> at EMNLP-IJCNLP 2019. <a href="https://www.aclweb.org/anthology/D19-6403.pdf" target="_blank">[paper]</a> <a href="https://www.aclweb.org/anthology/D19-6403.bib" target="_blank">[bib]</a> <a href="https://github.com/sandropezzelle/malevic" target="_blank">[github]</a></li>

<br>

			<li>[15] <b>Pezzelle, S.</b>, Fernández, R. (2019). Is the <i>Red Square</i> Big? MALeViC: Modeling Adjectives Leveraging Visual Contexts. <b>EMNLP-IJCNLP 2019</b>. <a href="https://www.aclweb.org/anthology/D19-1285.pdf" target="_blank">[paper]</a> <a href="https://www.aclweb.org/anthology/D19-1285.bib" target="_blank">[bib]</a> <a href="https://github.com/sandropezzelle/malevic" target="_blank">[github]</a></li>

<br>

                        <li>[14] Testoni, A., <b>Pezzelle, S.</b>, Bernardi, R. (2019). Quantifiers in a Multimodal World: Hallucinating Vision with Language and Sound. <b>CMCL 2019</b> at NAACL-HLT 2019. <a href="https://www.aclweb.org/anthology/W19-2912" target="_blank">[paper]</a> <a href="https://www.aclweb.org/anthology/W19-2912.bib" target="_blank">[bib]</a></li>

<br>

<!--
			</ul>
			<h2>2018</h2>
			<ul>
-->

			<li>[13] <b>Pezzelle, S.</b>, Bernardi, R., Piazza, M. (2018). Probing the Mental Representation of Quantifiers. <b>Cognition</b>, 181, 117-126. <a href="https://www.sciencedirect.com/science/article/pii/S0010027718302130" target="_blank">[paper]</a> <a href="https://iris.unitn.it/retrieve/handle/11572/212630/465340/Pezzelle_etal_Probing_accepted.pdf" target="_blank">[preprint]</a> <a href="bib/cognition.bib" target="_blank">[bib]</a></li>

<br>

			<li>[12] <b>Pezzelle, S.</b>, Steinert-Threlkeld, S., Bernardi, R., Szymanik, J. (2018). <i>Some of</i> them can Be Guessed! Exploring the Effect of Linguistic Context in Predicting Quantifiers, <b>ACL 2018</b>. <a href="http://aclweb.org/anthology/P18-2019" target="_blank">[paper]</a> <a href="https://www.aclweb.org/anthology/P18-2019.bib" target="_blank">[bib]</a> <a href="https://arxiv.org/pdf/1806.00354.pdf" target="_blank">[arxiv]</a> <a href="https://github.com/sandropezzelle/fill-in-the-quant" target="_blank">[github]</a> <a href="https://www.aclweb.org/anthology/attachments/P18-2019.Poster.pdf" target="_blank">[poster]</a></li>

<br>


			<li>[11] <b>Pezzelle, S.</b>, Sorodoc, I., Bernardi, R. (2018). Comparatives, Quantifiers, Proportions: A Multi-Task Model for the Learning of Quantities from Vision, <b>NAACL-HLT 2018</b>. <a href="http://aclweb.org/anthology/N18-1039" target="_blank">[paper]</a> <a href="bib/naacl2018.bib" target="_blank">[bib]</a> <a href="https://github.com/sandropezzelle/multitask-quant" target="_blank">[github]</a> <a href="Pezzelle_NAACL2018.pdf" target="_blank">[slides]</a> <a href="poster_caos_def.pdf" target="_blank">[poster]</a> <a href="https://arxiv.org/pdf/1804.05018.pdf" target="_blank">[arxiv]</a></li>

<br>

			<li>[10] Sorodoc, I., <b>Pezzelle, S.</b>, Dimiccoli, M., Herbelot, A., Bernardi, R. (2018). Learning Quantification from Images: A Structured Neural Architecture. <b>JNLE 2018</b>. <a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/FCD108133CB2030B785366DBF529A892/S1351324918000128a.pdf/learning_quantification_from_images_a_structured_neural_architecture.pdf" target="_blank">[paper]</a> <a href="bib/jnle.bib" target="_blank">[bib]</a> [github] <a href="https://arxiv.org/pdf/1704.02923.pdf" target="_blank">[arxiv]</a></li>

<br>

<!--
			</ul>
                        <h2>2017</h2>
                        <ul>
-->
    
                    <li>[9] Smith, D. A., <b>Pezzelle, S.</b>, Franzon, F., Zanini, C., Bernardi, R. (2017). Can you See the (Linguistic) Difference? Exploring the Mass/Count Distinction in Vision, <b>IWCS 2017</b>. <a href="http://aclweb.org/anthology/W17-6939" target="_blank">[paper]</a> <a href="">[poster]</a></li>

<br>

                        <li>[8] Shekhar, R., <b>Pezzelle, S.</b>, Herbelot, A., Nabi, M., Sangineto, E., Bernardi, R. (2017). Vision and Language Integration: Moving beyond Objects, <b>IWCS 2017</b>. <a href="https://foilunitn.github.io/" target="_blank">[webpage]</a> <a href="http://www.aclweb.org/anthology/W17-6938" target="_blank">[paper]</a></li>

<br>

                        <li>[7] Shekhar, R., <b>Pezzelle, S.</b>, Klimovich, Y., Herbelot, A., Nabi, M., Sangineto, E., Bernardi, R. (2017). FOIL it! Find One mismatch between Image and Language caption, <b>ACL 2017</b>. <a href="https://foilunitn.github.io/" target="_blank">[webpage]</a> <a href="https://arxiv.org/pdf/1705.01359.pdf" target="_blank">[paper]</a></li> 

<br>

                        <li>[6] <b>Pezzelle, S.</b>, Marelli, M., Bernardi, R. (2017). Be Precise or Fuzzy: Learning the Meaning of Cardinals and Quantifiers from Vision, <b>EACL 2017</b>. <a href="http://aclweb.org/anthology/E17-2054" target="_blank">[paper]</a> <a href="bib/beprecise.bib" target="_blank">[bib]</a> <a href="/Publications/poster_EACL.pdf" target="_blank">[poster]</a> <a href="/Publications/Be_precise_or_fuzzy.pdf">[slides]</a> <a href="https://arxiv.org/pdf/1702.05270.pdf" target="_blank">[arxiv]</a></li>                    

<br>   

<!--
                     </ul>
                        <h2>2016</h2>
                        <ul>
-->

                        <li>[5] <b>Pezzelle, S.</b>, Sorodoc, I., Herbelot, A., Bernardi, R. (2016). Imparare a quantificare guardando, <b>CLIC-it 2016</b>. <a href="http://ceur-ws.org/Vol-1749/paper42.pdf" target="_blank">[paper]</a> <a href="/Publications/clic-it2016.pdf" target="_blank">[slides]</a></li>

<br>

                        <li>[4] Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q., Bernardi, R., <b>Pezzelle, S.</b>, Baroni, M., Boleda, G., and Fernández, R. (2016). The LAMBADA dataset: Word prediction requiring a broad discourse context, <b>ACL 2016</b>. <a href="https://www.aclweb.org/anthology/P/P16/P16-1144.pdf" target="_blank">[paper]</a> <a href="http://clic.cimec.unitn.it/lambada/" target="_blank">[webpage]</a></li>

<br>

                        <li>[3] <b>Pezzelle, S.</b>, Shekhar, R., Bernardi, R. (2016). Building a bagpipe with a bag and a pipe: Exploring conceptual combination in Vision, <b>VL 2016</b> at ACL 2016. <a href="http://www.aclweb.org/anthology/W/W16/W16-3208.pdf" target="_blank">[paper]</a> <a href="https://github.com/shekharRavi/nn-composition" target="_blank">[data]</a> <a href="/Publications/bagpipe_poster2016.pdf" target="_blank">[poster]</a> <a href="/Publications/bagpipe_slides2016.pdf" target="_blank">[slides]</a></li>

<br>

                        <li>[2] Sorodoc, I., Lazaridou, A., Boleda, G., Herbelot, A., <b>Pezzelle, S.</b>, Bernardi, R. (2016). 'Look, some green circles!': Learning to quantify from images, <b>VL 2016</b> at ACL 2016. <a href="https://aclweb.org/anthology/W/W16/W16-3211.pdf" target="_blank">[paper]</a> <a href="circles_poster2016.pdf" target="_blank">[poster]</a> <a href="circles_slides2016.pdf" target="_blank">[slides]</a></li>

<br>

<!--
                        </ul>
                        <h2>2015</h2>
                        <ul>
-->

                        <li>[1] <b>Pezzelle, S.</b> (2015) Lorenzo Da Ponte: Metro e Stile delle Poesie del Periodo Americano, Stilistica e Metrica Italiana XV, Edizioni del Galluzzo, pages 83-120, Firenze, Italy, 2015. <a href="http://www.sismel.it/tilist.asp?hdncolragid=2&hdncol=smi" target="_blank">[webpage]</a></li>

                     </ul>
                     <h2>Abstracts</h2>
			<ul>

			<li>Takmaz, E., <b>Pezzelle, S.</b>, Fernández, R. (2022). Time Alignment between Gaze and Speech in Image Descriptions: Exploring Theories of Linearization. <b>CogSci 2022</b>.</li>

<br>

			<li><b>Pezzelle, S.</b>, Fernández, R. (2020). Asking questions with a big impact: Adapting to other interpretations of gradable adjectives. <b>CogSci 2020</b>.</li>
<br>
			<li>Takmaz, E., Beinborn, L., <b>Pezzelle, S.</b>, Fernández, R. (2019). Enhancing Neural Image Captioning with Eye-Tracking. <b>EurNLP 2019</b>.</li>
<br>
			

			<li><b>Pezzelle, S.</b>, Greco, C., Herbelot, A., Klein, T., Nabi, M., Bernardi, R. (2018). Be Different to Be Better: Toward the Integration of Vision and Language. <b>SiVL 2018</b> at ECCV 2018.</li>

<br>

<li>Jolly, S., <b>Pezzelle, S.</b>, Klein, T., Dengel, A., Nabi, N. (2018). An Evaluative Look at the Evaluation of VQA. <b>SiVL 2018</b> at ECCV 2018.</li>

<br>			
                        
                        
                        <li><b>Pezzelle, S.</b>, Jezek, E., Micheli, M. S. (2017). The different meanings of 'a': Capturing qualia relations of Italian complex nominals with distributional semantics, Workshop on the Role of Constituents in Multi-Word Expressions at <b>DGfS 2017</b>. <a href="http://www.ims.uni-stuttgart.de/events/dgfs-mwe-17/abstracts/pezzele-et-al_dgfs-mwe-2017.pdf" target="_blank">[abstract]</a></li>
                        </ul>

<!--

                     <h2>Preprints</h2>

			<ul>
                        <li>Cinà, G., Fernandez-Llaneza, D., Mishra, N., Röber, T. E., <b>Pezzelle, S.</b>, Calixto, I., Goedhart, R. and Birbil, Ş. İ <i>(submitted)</i>. Fixing confirmation bias in feature attribution methods via semantic match. <a href="https://arxiv.org/pdf/2307.00897.pdf" target="_blank">[preprint]</a></li>
			
			<br>

			<li>Jolly, S.*, <b>Pezzelle, S.*</b>, Klein, T., Dengel, A., Nabi, M. (2018). The Wisdom of MaSSeS: Majority, Subjectivity, and Semantic Similarity in the Evaluation of VQA, arXiv:1809.04344. <a href="https://arxiv.org/pdf/1809.04344.pdf" target="_blank">[arxiv]</a> <a href="https://sapmlresearch.github.io/MaSSeS/" target="_blank">[data&code]</a></li>
			</ul>

-->

                </div><!-- /.blurb -->
                </div><!-- /.container -->
                <footer>
                <ul>
        		<li><a href="mailto:s dot pezzelle at uva dot nl">email</a></li>
                        <li><a href="https://twitter.com/sandropezzelle" target="_blank">twitter</a></li>
                        <li><a href="https://www.researchgate.net/profile/Sandro_Pezzelle" target="_blank">researchgate</a></li>
                        <li><a href="https://scholar.google.it/citations?user=PW6eQ6YAAAAJ&hl=en&oi=ao" target="_blank">scholar</a></li>
                        <li><a href="https://www.linkedin.com/in/sandropezzelle/" target="_blank">linkedin</a></li>
        		<li><a href="https://github.com/sandropezzelle" target="_blank">github.com/sandropezzelle</a></li>
                        </ul>
                </footer>
        </body>
</html>

